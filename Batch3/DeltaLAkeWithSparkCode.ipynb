{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ad9cc7-ba5a-402f-a9db-d00801a9bb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.environ[\"PATH\"] = os.path.dirname(sys.executable) + os.pathsep + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea86dce-068c-4b7d-8b26-56ade85078e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\krishna\\anaconda3\\envs\\spark39\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = r\"C:\\Users\\krishna\\anaconda3\\envs\\spark39\\python.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c13f975c-b859-4bb6-b1ac-84539843d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b243960-cd2a-4c14-9538-b8a343de2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"DeltaApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c90c1d1-5418-47c8-85af-e8267f2e5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc48bc4d-93f2-4d6f-a91a-640f742a9a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0x2276dbdcd90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeltaTable.create(spark)\\\n",
    ".tableName(\"employee_demo\")\\\n",
    ".addColumn(\"emp_id\",\"LONG\")\\\n",
    ".addColumn(\"ename\",\"STRING\")\\\n",
    ".addColumn(\"gender\",\"STRING\")\\\n",
    ".addColumn(\"salary\",\"LONG\")\\\n",
    ".addColumn(\"dept\",\"LONG\")\\\n",
    ".property(\"description\",\"table created\")\\\n",
    ".location(r\"C:\\Users\\Krishna\\data\\empdata\")\\\n",
    ".execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e2b699-857c-4ff0-a7d7-0c7c9ead6326",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(100,'Ram','M',5000,10),(200,'Sia','F',4500,20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "736f836f-4947-4eea-9eb6-e50938593701",
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = [\"emp_id\",\"ename\",\"gender\",\"salary\",\"dept\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57840c5a-662f-4ec1-83ad-f4a732af8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark.createDataFrame(data=data,schema=sch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6b96c42-36c5-4312-87c7-13350b795e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+----+\n",
      "|emp_id|ename|gender|salary|dept|\n",
      "+------+-----+------+------+----+\n",
      "|   100|  Ram|     M|  5000|  10|\n",
      "|   200|  Sia|     F|  4500|  20|\n",
      "+------+-----+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14a37d5-a1d5-4207-8d19-f271724a35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('delta').mode(\"append\").saveAsTable(\"employee_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9bb7bcf-8601-4ecd-afb9-996f7f3f8f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = DeltaTable.forName(spark,'employee_Demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33ca8f99-7ff8-4178-9875-dfa3c1266215",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb1 = DeltaTable.forPath(spark,r'C:\\Users\\Krishna\\data\\empdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e6696d0-04b9-452b-9ab3-810d393643b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+----+\n",
      "|emp_id|ename|gender|salary|dept|\n",
      "+------+-----+------+------+----+\n",
      "|   200|  Sia|     F|  4500|  20|\n",
      "|   100|  Ram|     M|  5000|  10|\n",
      "+------+-----+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dc8d117-d21a-4938-a9b1-eef156338337",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.delete('emp_id =100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74640214-167e-4b83-9525-e359a8c34710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+----+\n",
      "|emp_id|ename|gender|salary|dept|\n",
      "+------+-----+------+------+----+\n",
      "|   200|  Sia|     F|  4500|  20|\n",
      "+------+-----+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "888e6558-a670-466f-a6fa-df2fcbdbe23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      2|2025-08-25 16:24:...|  null|    null|      DELETE|{predicate -> [\"(...|null|    null|     null|          1|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      1|2025-08-25 15:54:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          0|  Serializable|         true|{numFiles -> 3, n...|        null|Apache-Spark/3.3....|\n",
      "|      0|2025-08-25 15:54:...|  null|    null|CREATE TABLE|{isManaged -> fal...|null|    null|     null|       null|  Serializable|         true|                  {}|        null|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e1dfbd6-8e79-4a3a-9477-b860c64b063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(300,'Ravan','M',5000,10),(400,'Hanuman','m',4500,20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3edf95a1-8646-4471-b7e1-f5010c94122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = [\"emp_id\",\"ename\",\"gender\",\"salary\",\"dept\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a76c53a-7460-4a53-ac00-e18607cae75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf= spark.createDataFrame(data=data,schema=ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9adc3885-1f3d-4644-b418-d596887e3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.write.insertInto('employee_demo',overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ed9e440-f97e-4a16-9329-881eaabf2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = DeltaTable.forName(spark,'employee_Demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4db2b522-0189-4ae9-8fdf-a8ed8e446fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+----+\n",
      "|emp_id|  ename|gender|salary|dept|\n",
      "+------+-------+------+------+----+\n",
      "|   400|Hanuman|     m|  4500|  20|\n",
      "|   300|  Ravan|     M|  5000|  10|\n",
      "|   900|   John|     M|  8000|  10|\n",
      "|   900|   John|     M|  8000|  10|\n",
      "|   200|    Sia|     F|  4500|  20|\n",
      "+------+-------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "334657cb-3bf0-4fa1-82b4-7cf5290ca143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------------+\n",
      "|version|              timestamp|   operation|\n",
      "+-------+-----------------------+------------+\n",
      "|      9|2025-08-25 17:04:07.198|      UPDATE|\n",
      "|      8|2025-08-25 17:03:55.784|      UPDATE|\n",
      "|      7|2025-08-25 17:03:43.674|      UPDATE|\n",
      "|      6|2025-08-25 16:56:33.272|      UPDATE|\n",
      "|      5| 2025-08-25 16:50:51.63|       WRITE|\n",
      "|      4| 2025-08-25 16:50:13.44|       WRITE|\n",
      "|      3|2025-08-25 16:40:13.149|       WRITE|\n",
      "|      2|2025-08-25 16:24:28.854|      DELETE|\n",
      "|      1| 2025-08-25 15:54:28.54|       WRITE|\n",
      "|      0|2025-08-25 15:54:06.561|CREATE TABLE|\n",
      "+-------+-----------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb.history().select('version','timestamp','operation').show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d999664-af08-42a5-997a-4253c660370e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e307685-7b0e-4713-aec4-a84c0940cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.toDF().createTempView('emp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a9b9800-6d4f-45ed-bb6c-5339468c57b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employee_demo', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='emp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "361a73e5-3f58-4cad-9949-376b4993ebf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"insert into employee_demo(emp_id,ename,gender,salary,dept) values(900,'SMith','M',4900,10)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fb7cb21-0983-45a9-a4b2-7c7dbe63b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                2|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"update employee_demo set ename='John' where ename='SMith'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e47e29bf-b464-47b7-b5f5-24281d53e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.update(condition = \"ename = 'John' \",set={\"salary\": \"8000\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98d21137-4238-4c38-ac6c-6af409836773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+----+\n",
      "|emp_id|ename|gender|salary|dept|\n",
      "+------+-----+------+------+----+\n",
      "|   200|  Sia|     F|  4500|  20|\n",
      "+------+-----+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#time travel\n",
    "spark.sql('select * from employee_demo  version as of 2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7fcdb45-42c6-4b54-b31c-e4f8fd3234e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+----+\n",
      "|emp_id|  ename|gender|salary|dept|\n",
      "+------+-------+------+------+----+\n",
      "|   400|Hanuman|     m|  4500|  20|\n",
      "|   300|  Ravan|     M|  5000|  10|\n",
      "|   900|  SMith|     M|  4900|  10|\n",
      "|   200|    Sia|     F|  4500|  20|\n",
      "+------+-------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from employee_demo  timestamp as of '2025-08-25 16:50:51.6'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "734f84ca-cb74-42f9-a229-e4093d5e2309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
      "|table_size_after_restore|num_of_files_after_restore|num_removed_files|num_restored_files|removed_files_size|restored_files_size|\n",
      "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
      "|                    7275|                         5|                0|                 0|                 0|                  0|\n",
      "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb.restoreToVersion(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4dc7e8d1-12a9-4b51-9535-7a02c20f7245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+----+\n",
      "|emp_id|  ename|gender|salary|dept|\n",
      "+------+-------+------+------+----+\n",
      "|   400|Hanuman|     m|  4500|  20|\n",
      "|   900|  SMith|     M|  4900|  10|\n",
      "|   300|  Ravan|     M|  5000|  10|\n",
      "|   900|  SMith|     M|  4900|  10|\n",
      "|   200|    Sia|     F|  4500|  20|\n",
      "+------+-------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dcb96abb-25ec-4578-984a-122db794e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema Evolution\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7b3227a-ff8b-446d-b818-cc1e800c82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = StructType([\\\n",
    "StructField(\"emp_id\",LongType(),True),\\\n",
    "StructField(\"ename\",StringType(),True),\\\n",
    "StructField(\"gender\",StringType(),True),\\\n",
    "StructField(\"salary\",LongType(),True),\\\n",
    "StructField(\"dept\",LongType(),True),\\\n",
    "StructField(\"addcol1\",StringType(),True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2b7a54b7-25cd-41ef-8aa5-6b3483f3db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata =[(1000,'Cup','N',4567,30,'good')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5deb983-5ba1-4323-a88d-72fed73f191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = spark.createDataFrame(data=newdata,schema=sch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c1017470-a5d9-474d-9ee8-511b37ff7e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+----+-------+\n",
      "|emp_id|ename|gender|salary|dept|addcol1|\n",
      "+------+-----+------+------+----+-------+\n",
      "|  1000|  Cup|     N|  4567|  30|   good|\n",
      "+------+-----+------+------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "97cafa47-a92c-4338-babc-5ec2bf95bb28",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 13c07e7b-f59c-40c9-a3b1-910e645c5a65).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- emp_id: long (nullable = true)\n-- ename: string (nullable = true)\n-- gender: string (nullable = true)\n-- salary: long (nullable = true)\n-- dept: long (nullable = true)\n\n\nData schema:\nroot\n-- emp_id: long (nullable = true)\n-- ename: string (nullable = true)\n-- gender: string (nullable = true)\n-- salary: long (nullable = true)\n-- dept: long (nullable = true)\n-- addcol1: string (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# will fail\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnewdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memployee_demo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\spark39\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1041\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1041\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\spark39\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\spark39\\lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 13c07e7b-f59c-40c9-a3b1-910e645c5a65).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- emp_id: long (nullable = true)\n-- ename: string (nullable = true)\n-- gender: string (nullable = true)\n-- salary: long (nullable = true)\n-- dept: long (nullable = true)\n\n\nData schema:\nroot\n-- emp_id: long (nullable = true)\n-- ename: string (nullable = true)\n-- gender: string (nullable = true)\n-- salary: long (nullable = true)\n-- dept: long (nullable = true)\n-- addcol1: string (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "# will fail\n",
    "newdf.write.format('delta').mode('append')\\\n",
    ".saveAsTable('employee_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c8455fce-0964-4c63-9ddd-d8b06dc0ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.write.format('delta').mode('append')\\\n",
    ".option('mergeSchema',\"true\").saveAsTable('employee_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3fb8488-e62d-4869-b4c9-363cf9b8559e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+----+-------+\n",
      "|emp_id|  ename|gender|salary|dept|addcol1|\n",
      "+------+-------+------+------+----+-------+\n",
      "|  1000|    Cup|     N|  4567|  30|   good|\n",
      "|  1000|    Cup|     N|  4567|  30|   good|\n",
      "|   400|Hanuman|     m|  4500|  20|   null|\n",
      "|   900|  SMith|     M|  4900|  10|   null|\n",
      "|   300|  Ravan|     M|  5000|  10|   null|\n",
      "|   900|  SMith|     M|  4900|  10|   null|\n",
      "|   200|    Sia|     F|  4500|  20|   null|\n",
      "+------+-------+------+------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tbnew = DeltaTable.forName(spark,'employee_demo')\n",
    "tbnew.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "34b6fc5c-2cb9-4b8a-a78c-40796c01c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = StructType([\\\n",
    "StructField(\"emp_id\",LongType(),True),\\\n",
    "StructField(\"ename\",StringType(),True),\\\n",
    "StructField(\"gender\",StringType(),True),\\\n",
    "StructField(\"salary\",LongType(),True),\\\n",
    "StructField(\"dept\",LongType(),True),\\\n",
    "StructField(\"addcol2\",StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8977d100-dc2c-4ab7-92a9-3b60b76d65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata =[(2000,'pen','N',4567,30,'good')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "69885d8e-3f9d-4cd5-b882-84a00518387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = spark.createDataFrame(data=newdata,schema=sch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c8c0a817-bf04-40dd-8598-49857ddc8a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+----+-------+\n",
      "|emp_id|ename|gender|salary|dept|addcol2|\n",
      "+------+-----+------+------+----+-------+\n",
      "|  2000|  pen|     N|  4567|  30|   good|\n",
      "+------+-----+------+------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ae7af737-1834-4441-a4a1-639e311a3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.write.format('delta').mode('append')\\\n",
    ".option('mergeSchema',\"true\").saveAsTable('employee_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4135095a-6678-4911-a150-d90d9fdb3b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+----+-------+-------+\n",
      "|emp_id|  ename|gender|salary|dept|addcol1|addcol2|\n",
      "+------+-------+------+------+----+-------+-------+\n",
      "|  2000|    pen|     N|  4567|  30|   null|   good|\n",
      "|  1000|    Cup|     N|  4567|  30|   good|   null|\n",
      "|  1000|    Cup|     N|  4567|  30|   good|   null|\n",
      "|   400|Hanuman|     m|  4500|  20|   null|   null|\n",
      "|   900|  SMith|     M|  4900|  10|   null|   null|\n",
      "|   300|  Ravan|     M|  5000|  10|   null|   null|\n",
      "|   900|  SMith|     M|  4900|  10|   null|   null|\n",
      "|   200|    Sia|     F|  4500|  20|   null|   null|\n",
      "+------+-------+------+------+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tbnew = DeltaTable.forName(spark,'employee_demo')\n",
    "tbnew.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1c22e672-c383-40e0-99c5-89813958fa0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb1.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aeb07df5-7d95-443e-af65-2beef52c4e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|     15|2025-08-26 12:06:...|  null|    null|    OPTIMIZE|{predicate -> [],...|null|    null|     null|         14|SnapshotIsolation|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|     14|2025-08-26 11:08:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|         13|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|     13|2025-08-26 11:06:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|         12|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|     12|2025-08-26 11:03:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|         11|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|     11|2025-08-25 22:38:...|  null|    null|     RESTORE|{version -> 5, ti...|null|    null|     null|         10|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.3....|\n",
      "|     10|2025-08-25 22:37:...|  null|    null|     RESTORE|{version -> 5, ti...|null|    null|     null|          9|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.3....|\n",
      "|      9|2025-08-25 17:04:...|  null|    null|      UPDATE|{predicate -> (en...|null|    null|     null|          8|     Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      8|2025-08-25 17:03:...|  null|    null|      UPDATE|{predicate -> (en...|null|    null|     null|          7|     Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      7|2025-08-25 17:03:...|  null|    null|      UPDATE|{predicate -> (en...|null|    null|     null|          6|     Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      6|2025-08-25 16:56:...|  null|    null|      UPDATE|{predicate -> (en...|null|    null|     null|          5|     Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      5|2025-08-25 16:50:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          4|     Serializable|         true|{numFiles -> 1, n...|        null|Apache-Spark/3.3....|\n",
      "|      4|2025-08-25 16:50:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          3|     Serializable|         true|{numFiles -> 1, n...|        null|Apache-Spark/3.3....|\n",
      "|      3|2025-08-25 16:40:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          2|     Serializable|         true|{numFiles -> 3, n...|        null|Apache-Spark/3.3....|\n",
      "|      2|2025-08-25 16:24:...|  null|    null|      DELETE|{predicate -> [\"(...|null|    null|     null|          1|     Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      1|2025-08-25 15:54:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          0|     Serializable|         true|{numFiles -> 3, n...|        null|Apache-Spark/3.3....|\n",
      "|      0|2025-08-25 15:54:...|  null|    null|CREATE TABLE|{isManaged -> fal...|null|    null|     null|       null|     Serializable|         true|                  {}|        null|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb1.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9150ed81-9136-4120-9c65-3a933e3b4749",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 16-17: truncated \\UXXXXXXXX escape (854032216.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[97], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    spark.sql(\"VACUUM delta.'C:\\Users\\Krishna\\data\\empdata' DRY RUN\")\u001b[0m\n\u001b[1;37m                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 16-17: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# Dry run: Just see what would be deleted without actually deleting anything\n",
    "spark.sql(\"VACUUM delta.'C:\\Users\\Krishna\\data\\empdata' DRY RUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2ee6bbb0-d8bd-4250-8df3-d128a8d38853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb1.vacuum(retentionHours=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d4638f6d-7308-49fc-9ece-a7ed261a73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To change the retention period, you need to modify the Spark configuration. However, Delta Lake enforces a minimum 168 hours \n",
    "#(7 days) retention period unless you explicitly override it.\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5235c5-acfb-4f11-a930-bb9aa385528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"VACUUM delta.`dbfs:/mnt/delta/my_table` RETAIN 2 HOURS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark39)",
   "language": "python",
   "name": "spark39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
